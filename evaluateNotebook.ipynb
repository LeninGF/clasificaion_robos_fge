{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5487c8cd-079a-4a64-9eaf-9a84ea79cd10",
   "metadata": {},
   "source": [
    "## Evaluate notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4571a5f2-aa48-47c7-b940-d4d0be6b5df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import os\n",
    "dir_root = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acea8f94-4d6e-4e87-867f-30c3018f5b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
      "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
      " Where:\n",
      "TP: True positive\n",
      "TN: True negative\n",
      "FP: False positive\n",
      "FN: False negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_ev = evaluate.load(\"accuracy\")\n",
    "print(accuracy_ev.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "185710bc-cdec-4e18-bfe7-b63486a7092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# model_tf = tf.saved_model.load(os.path.join(os.getcwd(), 'models/tfsample/'))\n",
    "# model_tf = tf.keras.models.load_model(os.path.join(os.getcwd(), 'models/modeltf/'))\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_tf = AutoModelForSequenceClassification.from_pretrained('models/huggingfacemodel/', from_tf=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e610f38-d36f-43c6-a363-be76b1668d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0166ee4b-aeeb-4833-b768-b367b6f7c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(os.path.join(dir_root, 'data/interim/trainset.csv'), converters={'NDD':str})\n",
    "labels_set = set(dataset_df.labels.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da140d8c-a678-422d-8c0b-b14a410aff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Features, ClassLabel, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b31fe5a-da84-4ff8-a152-6b1debf2b31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-92b60acd05f883de\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\entea\\.cache\\huggingface\\datasets\\csv\\default-92b60acd05f883de\\0.0.0\\51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 2998.79it/s]\n",
      "Extracting data files: 100%|████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 499.90it/s]\n",
      "                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\entea\\.cache\\huggingface\\datasets\\csv\\default-92b60acd05f883de\\0.0.0\\51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 49.97it/s]\n"
     ]
    }
   ],
   "source": [
    "traincsv = os.path.join(dir_root, 'data/interim/trainsethugf.csv')\n",
    "testcsv = os.path.join(dir_root, 'data/interim/testsethugf.csv')\n",
    "validcsv = os.path.join(dir_root, 'data/interim/validsethugf.csv')\n",
    "# class_names = [\"RoboADomicilio\", \"RoboAPersonas\", \"RoboAUnidadesEconomicas\", \"RoboDeBienesAccesoriosYAutoPartes\", \"RoboDeCarros\", \"RoboDeMotos\"]\n",
    "class_names = list(labels_set)\n",
    "robo_features = Features({'relato': Value('string'), 'labels': ClassLabel(names=class_names)})\n",
    "dataset = load_dataset(\"csv\", data_files={'train': traincsv, 'test': testcsv, 'validation':validcsv}, features=robo_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce9fa277-7248-419e-88bb-1521e331f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = dataset_df.relato.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa55cbf9-6596-473f-9689-78350e309ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, AutoTokenizer, DistilBertTokenizerFast\n",
    "# model_name = 'xlm-roberta-large'\n",
    "# model_name = 'bert-base-cased'\n",
    "# model_name = 'bert-base-multilingual-uncased-sentiment'\n",
    "model_name = 'distilbert-base-multilingual-cased'\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b2a9cf5-313d-4dbb-8f99-2da5b6025eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 274/274 [01:04<00:00,  4.22ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 90/90 [00:22<00:00,  4.09ba/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 69/69 [00:16<00:00,  4.08ba/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenizer_func(examples):\n",
    "  return tokenizer(examples[\"relato\"],\n",
    "                   max_length=seqlen.max(),\n",
    "                   padding = \"max_length\",\n",
    "                   truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenizer_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b055c99-bb5b-49a4-8dc5-ef03d25e3e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2b034f4-93ac-45ec-8fc9-33f8529f8644",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_test_set = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(4000)).to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91ff5550-a221-4c78-aaca-8f2086b5b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import create_optimizer\n",
    "\n",
    "# batch_size = 16\n",
    "# num_epochs = 50\n",
    "# batches_per_epoch = len(tokenized_dataset[\"train\"]) // batch_size\n",
    "# total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "# optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f6359bf-5ecb-48ef-a36c-d52d1b245192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_tf.compile(optimizer=optimizer, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc51a246-95a1-4e04-94b6-b9f2700cd4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_tf.evaluate(tf_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8099e21c-47fc-42e1-bb90-bd7143dc7d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\entea\\.cache\\huggingface\\datasets\\csv\\default-92b60acd05f883de\\0.0.0\\51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\\cache-1dfcb5a6ce1efb95.arrow\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'LABEL_2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m metric \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28meval\u001b[39m \u001b[38;5;241m=\u001b[39m evaluator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelato\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\evaluate\\evaluator.py:278\u001b[0m, in \u001b[0;36mTextClassificationEvaluator.compute\u001b[1;34m(self, model_or_pipeline, data, metric, tokenizer, strategy, confidence_level, n_resamples, random_state, input_column, label_column, label_mapping)\u001b[0m\n\u001b[0;32m    276\u001b[0m references \u001b[38;5;241m=\u001b[39m data[label_column]\n\u001b[0;32m    277\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_predictions(pipe, data[input_column], label_mapping\u001b[38;5;241m=\u001b[39mlabel_mapping)\n\u001b[1;32m--> 278\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbootstrap\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    281\u001b[0m     metric_keys \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\evaluate\\module.py:425\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\evaluate\\module.py:478\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    477\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enforce_nested_string_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_features[key], obj) \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[1;32m--> 478\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mwrite_batch(batch)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\datasets\\features\\features.py:1596\u001b[0m, in \u001b[0;36mFeatures.encode_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m   1594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1595\u001b[0m     column \u001b[38;5;241m=\u001b[39m cast_to_python_objects(column)\n\u001b[1;32m-> 1596\u001b[0m     encoded_batch[key] \u001b[38;5;241m=\u001b[39m [encode_nested_example(\u001b[38;5;28mself\u001b[39m[key], obj) \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\datasets\\features\\features.py:1596\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   1595\u001b[0m     column \u001b[38;5;241m=\u001b[39m cast_to_python_objects(column)\n\u001b[1;32m-> 1596\u001b[0m     encoded_batch[key] \u001b[38;5;241m=\u001b[39m [\u001b[43mencode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\datasets\\features\\features.py:1203\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[1;34m(schema, obj, level)\u001b[0m\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;66;03m# Object with special encoding:\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;66;03m# ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\u001b[39;00m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image, ClassLabel, TranslationVariableLanguages, Value, _ArrayXD)):\n\u001b[1;32m-> 1203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;66;03m# Other object should be directly convertible to a native Arrow type (like Translation and Translation)\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\datasets\\features\\features.py:465\u001b[0m, in \u001b[0;36mValue.encode_example\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(value)\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_integer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_type):\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_floating(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_type):\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(value)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'LABEL_2'"
     ]
    }
   ],
   "source": [
    "from evaluate import evaluator\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "eval = evaluator(\"text-classification\")\n",
    "\n",
    "results = eval.compute(model_tf, data=dataset[\"test\"].shuffle(seed=42).select(range(4000)), metric=metric, input_column=\"relato\", label_column=\"labels\", tokenizer = tokenizer)\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
