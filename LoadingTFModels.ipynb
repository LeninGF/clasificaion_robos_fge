{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c296bb-9630-41f6-a71c-3e421680c8c8",
   "metadata": {},
   "source": [
    "## Robos Pretrained Loader\n",
    "\n",
    "Este notebook tiene propósito cargar el modelo fine-tuned (ajustado) en Google Colab y el modelo fine tuned en mi PC y realizar las preubas de testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14fdc3-7722-423f-a20d-89379aa65d88",
   "metadata": {},
   "source": [
    "## Generación del Dataset de Prueba con load_dataset - tokenización - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe3f328-491c-4751-ab20-9d30ebb79b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "dir_root = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f2ec1f5-6c01-4473-a376-e8a8ae38bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(os.path.join(dir_root, 'data/interim/trainset.csv'), converters={'NDD':str})\n",
    "# dataset_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5519266-9113-4d3e-a8eb-8eb77031c358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ROBO A DOMICILIO',\n",
       " 'ROBO A PERSONAS',\n",
       " 'ROBO A UNIDADES ECONOMICAS',\n",
       " 'ROBO DE BIENES, ACCESORIOS Y AUTOPARTES DE VEHICULOS',\n",
       " 'ROBO DE CARROS',\n",
       " 'ROBO DE MOTOS'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_set = set(dataset_df.labels.to_list())\n",
    "labels_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2cb5ebd-f2c8-416a-86a3-0df70acbb072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using custom data configuration default-dff74bfcd49b570b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\entea\\.cache\\huggingface\\datasets\\csv\\default-dff74bfcd49b570b\\0.0.0\\51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 333.17it/s]\n",
      "                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\entea\\.cache\\huggingface\\datasets\\csv\\default-dff74bfcd49b570b\\0.0.0\\51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 66.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Features, ClassLabel, Value\n",
    "# traincsv = os.path.join(dir_root, '/content/drive/MyDrive/data/interim/trainsethugf.csv')\n",
    "# traincsv = os.path.join(dir_root, 'data/interim/trainsethugf.csv')\n",
    "# testcsv = os.path.join(dir_root, '/content/drive/MyDrive/data/interim/testsethugf.csv')\n",
    "testcsv = os.path.join(dir_root, 'data/interim/testsethugf.csv')\n",
    "# validcsv = os.path.join(dir_root, '/content/drive/MyDrive/data/interim/validsethugf.csv')\n",
    "# validcsv = os.path.join(dir_root, 'data/interim/validsethugf.csv')\n",
    "# class_names = [\"RoboADomicilio\", \"RoboAPersonas\", \"RoboAUnidadesEconomicas\", \"RoboDeBienesAccesoriosYAutoPartes\", \"RoboDeCarros\", \"RoboDeMotos\"]\n",
    "class_names = list(labels_set)\n",
    "robo_features = Features({'relato': Value('string'), 'labels': ClassLabel(names=class_names)})\n",
    "dataset = load_dataset(\"csv\", data_files={'test': testcsv}, features=robo_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee4fc98-9fdd-46b9-82b8-448db4b1a555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['relato', 'labels'],\n",
       "        num_rows: 90000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c0b07a4-dfc6-49f5-a820-d124648f5fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, AutoTokenizer, DistilBertTokenizerFast\n",
    "model_name = 'distilbert-base-multilingual-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c46bd29-7d5f-4fa6-b68f-42dc3fc55c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqlen = dataset_df.relato.apply(lambda x: len(x.split()))\n",
    "seqlen.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0016c0b-b4fa-430b-b380-1dc6d6f1d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_func(examples):\n",
    "  return tokenizer(examples[\"relato\"],\n",
    "                   max_length=seqlen.max(),\n",
    "                   padding = \"max_length\",\n",
    "                   truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b701f91-52fb-4917-a727-2b86b84de6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 90/90 [00:21<00:00,  4.17ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenizer_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22c499c-4aa0-48bf-8b05-1c5015db2465",
   "metadata": {},
   "source": [
    "## Generación del Dataset de Prueba con tensorflow slices - tokenización - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2879ab3-933b-4797-9adc-810f5cc6d738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 90841, 69229, 28854, 10124, 169, 17217, 14729, 10189, 17611, 28615, 33357, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Artificial Intelligence is a complex science  that sometimes needs philosophy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b919aec-0072-43e6-ab31-d273ef335e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b1080e2-478f-45ee-8022-903aa32449e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generando muestra de 4000 para tf_test_set\n",
    "tf_test_set = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(200)).to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d73bd087-39c4-486c-80c4-b423bde49246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': <tf.Tensor: shape=(16,), dtype=int64, numpy=array([0, 5, 0, 0, 5, 0, 5, 2, 2, 3, 1, 2, 0, 0, 0, 1], dtype=int64)>, 'input_ids': <tf.Tensor: shape=(16, 300), dtype=int64, numpy=\n",
      "array([[  101, 10125, 10671, ...,     0,     0,     0],\n",
      "       [  101, 10196, 10125, ..., 10162, 10215,   102],\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0]], dtype=int64)>, 'attention_mask': <tf.Tensor: shape=(16, 300), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int64)>}\n",
      "{'labels': <tf.Tensor: shape=(16,), dtype=int64, numpy=array([3, 5, 0, 2, 3, 0, 5, 2, 1, 2, 3, 2, 2, 2, 1, 3], dtype=int64)>, 'input_ids': <tf.Tensor: shape=(16, 300), dtype=int64, numpy=\n",
      "array([[  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       [  101, 10196, 10125, ..., 46975, 10110,   102],\n",
      "       ...,\n",
      "       [  101, 10196, 46350, ..., 10846, 13207,   102],\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0]], dtype=int64)>, 'attention_mask': <tf.Tensor: shape=(16, 300), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int64)>}\n",
      "{'labels': <tf.Tensor: shape=(16,), dtype=int64, numpy=array([5, 0, 1, 0, 3, 0, 0, 5, 0, 2, 0, 0, 0, 4, 0, 0], dtype=int64)>, 'input_ids': <tf.Tensor: shape=(16, 300), dtype=int64, numpy=\n",
      "array([[  101, 10125, 13101, ...,     0,     0,     0],\n",
      "       [  101, 47599, 35109, ...,     0,     0,     0],\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       [  101, 10125, 14184, ..., 46506, 10810,   102]], dtype=int64)>, 'attention_mask': <tf.Tensor: shape=(16, 300), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1]], dtype=int64)>}\n",
      "{'labels': <tf.Tensor: shape=(16,), dtype=int64, numpy=array([0, 0, 0, 0, 1, 5, 3, 1, 0, 4, 0, 5, 5, 1, 1, 3], dtype=int64)>, 'input_ids': <tf.Tensor: shape=(16, 300), dtype=int64, numpy=\n",
      "array([[  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 47599, 43645, ...,     0,     0,     0],\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0]], dtype=int64)>, 'attention_mask': <tf.Tensor: shape=(16, 300), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int64)>}\n",
      "{'labels': <tf.Tensor: shape=(16,), dtype=int64, numpy=array([0, 0, 0, 2, 1, 0, 1, 5, 2, 2, 0, 3, 3, 0, 4, 1], dtype=int64)>, 'input_ids': <tf.Tensor: shape=(16, 300), dtype=int64, numpy=\n",
      "array([[  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       [  101, 47599, 10113, ...,     0,     0,     0],\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0],\n",
      "       [  101, 10196, 10125, ...,     0,     0,     0]], dtype=int64)>, 'attention_mask': <tf.Tensor: shape=(16, 300), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int64)>}\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tf_test_set):\n",
    "    print(batch)\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b4842-a755-4bad-a7c2-0de65c0b5303",
   "metadata": {},
   "source": [
    "## Generación del dataset con Slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f10c594-4c0c-4d2e-9e00-4a067c49f449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\entea\\.cache\\huggingface\\datasets\\csv\\default-dff74bfcd49b570b\\0.0.0\\51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\\cache-f8fc9d2bcda1ef73.arrow\n"
     ]
    }
   ],
   "source": [
    "test_samples = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(200)) #.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6239d81b-e6fc-4b80-b91f-945bb91dc908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5, 0, 0, 5, 0, 5, 2, 2, 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples['labels'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd92c48-f74a-469a-ad25-f77cefc393e8",
   "metadata": {},
   "source": [
    "Generando los feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4c18590-413a-4a3c-a300-f1689f9b3993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generando los feature vectors\n",
    "test_features = test_samples.remove_columns([\"relato\", \"labels\"]).with_format(\"tensorflow\") # use tokenized_dataset to use full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "761ed74e-4c66-4f04-b5f1-625e78f3856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = {x: test_features[x] for x in tokenizer.model_input_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "722ea26a-8c25-4f48-8cc8-2c930aad34af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(200, 300), dtype=int64, numpy=\n",
       " array([[  101, 10125, 10671, ...,     0,     0,     0],\n",
       "        [  101, 10196, 10125, ..., 10162, 10215,   102],\n",
       "        [  101, 10196, 10125, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101, 10125, 13101, ...,     0,     0,     0],\n",
       "        [  101, 47599, 43645, ...,     0,     0,     0],\n",
       "        [  101, 10125, 14184, ...,     0,     0,     0]], dtype=int64)>,\n",
       " 'attention_mask': <tf.Tensor: shape=(200, 300), dtype=int64, numpy=\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]], dtype=int64)>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e11c87-57d5-4657-bf81-09bb61a301c1",
   "metadata": {},
   "source": [
    "Generando las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c22e5a9-0b2d-4fa9-8edc-3083891f9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "test_labels = to_categorical(test_samples[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2feb46cd-dc59-4624-be16-674bbe6f67aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c001285-654f-4c3e-9226-9b21e64a3640",
   "metadata": {},
   "source": [
    "## Carga del Modelo\n",
    "Se prueban dos modelos. El entrenado en el computador personal y el entrenado en Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0981a7c-d65c-4a87-ab00-1e94fb6d5ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./models/robospretrained1000/ were not used when initializing TFDistilBertForSequenceClassification: ['dropout_39']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ./models/robospretrained1000/ and are newly initialized: ['dropout_119']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, TFDistilBertForSequenceClassification, TFAutoModel, AutoModelForSequenceClassification, AutoConfig\n",
    "config = AutoConfig.from_pretrained('./models/robospretrained1000/config.json')\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained('./models/robospretrained1000/', local_files_only=True, config=config)\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained('./models/robosweights1k/')\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('models/robospretrained1000/', from_tf=True, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6d7d6464-7115-4e6a-a62b-14b2ba1555fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('models/robossave/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64e6a863-9834-41ce-90fe-4110af598689",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DistilBertForSequenceClassification' object has no attribute 'compile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m metrics\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mCategoricalAccuracy()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# metrics=tf.keras.metrics.Accuracy()\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# model.compile(optimizer=optimizer, metrics = ['accuracy', 'AUC', 'Precision', 'Recall'])\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m(optimizer\u001b[38;5;241m=\u001b[39moptimizer, metrics \u001b[38;5;241m=\u001b[39m metrics, loss\u001b[38;5;241m=\u001b[39mloss)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1177\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1176\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1177\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DistilBertForSequenceClassification' object has no attribute 'compile'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "metrics=tf.keras.metrics.CategoricalAccuracy()\n",
    "# metrics=tf.keras.metrics.Accuracy()\n",
    "# model.compile(optimizer=optimizer, metrics = ['accuracy', 'AUC', 'Precision', 'Recall'])\n",
    "model.compile(optimizer=optimizer, metrics = metrics, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aadef65c-ab35-4782-9a7e-e3872a93640d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  134734080 \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  4614      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 135,329,286\n",
      "Trainable params: 135,329,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e774e-2e79-42e3-bc9f-9efad892e480",
   "metadata": {},
   "source": [
    "## Testeo del modelo en dataset cargado mediante huggingface load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c713043b-8065-44d0-ae17-12e1e2c23a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "test_npy = tfds.as_numpy(tf_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36e6605c-3eb6-4501-b665-3130d87269ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable = list(batch['labels'] for batch in test_npy)\n",
    "y_true = np.hstack(iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ac3a618-2a9d-49c5-af22-bc3a4ddcd3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dd59676-c692-4a1c-bb3a-1237c5da8969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.98787555e-01, 5.03528165e-03, 2.58802832e-03, 3.58307501e-04,\n",
       "        7.78779387e-01, 1.44513864e-02],\n",
       "       [2.71832943e-01, 1.13147618e-02, 2.37117463e-04, 1.82100819e-04,\n",
       "        3.92450429e-02, 6.77187979e-01],\n",
       "       [4.74163936e-03, 1.41619742e-01, 1.65109605e-01, 4.80766371e-02,\n",
       "        6.30666196e-01, 9.78618581e-03],\n",
       "       [9.76291373e-02, 1.36636853e-01, 1.62562221e-01, 4.52137917e-01,\n",
       "        1.50388047e-01, 6.45814987e-04],\n",
       "       [7.43349269e-02, 8.40020999e-02, 1.00435654e-03, 3.47870984e-04,\n",
       "        8.52096640e-03, 8.31789732e-01],\n",
       "       [3.25737484e-02, 3.34094688e-02, 7.89692551e-02, 1.00340759e-02,\n",
       "        8.16217542e-01, 2.87959483e-02],\n",
       "       [1.50968373e-01, 9.06565785e-02, 1.87952723e-03, 4.73754568e-04,\n",
       "        2.51263510e-02, 7.30895400e-01],\n",
       "       [1.73550844e-02, 4.03932557e-02, 1.68228269e-01, 3.73745002e-02,\n",
       "        7.35150874e-01, 1.49802875e-03],\n",
       "       [4.82812850e-03, 1.83567926e-02, 7.68390000e-01, 1.04308344e-01,\n",
       "        9.33034793e-02, 1.08132418e-02],\n",
       "       [2.82423408e-03, 4.97448593e-02, 2.04145685e-01, 6.18040077e-02,\n",
       "        6.76951826e-01, 4.52935137e-03]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_logits = model.predict(tf_test_set).logits\n",
    "yprob = tf.nn.softmax(y_logits, axis=-1).numpy()\n",
    "yprob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee26a080-2c4b-472c-b693-381c08994fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = tf.argmax(yprob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "380b29f4-1035-4305-ac97-d1384406e7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 6), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_onehot = tf.one_hot(yhat, depth=6, dtype=tf.float32)\n",
    "yhat_onehot[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b61ce718-b79a-4dfa-a24f-ebc619e557d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue_onehot = tf.one_hot(y_true, depth=6, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "648b19f8-ccb8-4a65-8def-8cf0b9ea3416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        79\n",
      "           1       0.62      0.48      0.54        27\n",
      "           2       0.69      0.55      0.61        33\n",
      "           3       0.00      0.00      0.00        15\n",
      "           4       0.05      0.50      0.08        10\n",
      "           5       0.77      0.75      0.76        36\n",
      "\n",
      "   micro avg       0.32      0.32      0.32       200\n",
      "   macro avg       0.35      0.38      0.33       200\n",
      "weighted avg       0.34      0.32      0.31       200\n",
      " samples avg       0.32      0.32      0.32       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import  classification_report\n",
    "print(classification_report(y_true=ytrue_onehot, y_pred=yhat_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29327671-e972-45cf-a078-219866705b4e",
   "metadata": {},
   "source": [
    "## Testeo del modelo en datset cargado mediante tensorflow slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b8a839e-1c03-45ee-bfd0-1cd329034bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = test_labels\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4baa521-885c-4283-ba83-649f9cf19491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00d6b872-1f55-4592-bc01-ee9a148f6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing inference with our model\n",
    "predictions = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e771cdb2-d6d1-476a-8cbc-f9f20b07523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting predicted logits to probabilities\n",
    "predictions = tf.nn.softmax(predictions.logits)\n",
    "\n",
    "# Extracting the indices with the highest probabilities\n",
    "predictions = tf.argmax(predictions, axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7b9fc02-0cac-4fb4-af06-7d74852d9e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 4, 3, 5, 4, 5, 4, 2, 4, 2, 2, 4, 4, 4, 1, 5, 5, 4, 4, 5, 4,\n",
       "       1, 1, 1, 2, 4, 2, 2, 3, 1, 5, 4, 4, 4, 4, 0, 4, 2, 5, 4, 2, 2, 4,\n",
       "       4, 2, 4, 4, 4, 4, 4, 4, 1, 5, 5, 4, 4, 4, 4, 5, 5, 4, 1, 1, 4, 4,\n",
       "       4, 1, 1, 4, 1, 5, 4, 2, 4, 0, 0, 4, 2, 5, 5, 5, 4, 4, 4, 4, 4, 1,\n",
       "       1, 4, 4, 2, 4, 4, 1, 4, 4, 1, 4, 4, 4, 4, 4, 4, 2, 5, 4, 2, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 3, 2, 5, 2, 4, 5, 0, 4, 4, 4, 2, 1, 1, 4, 4, 4,\n",
       "       4, 5, 4, 2, 1, 4, 2, 5, 2, 5, 4, 4, 4, 1, 4, 2, 5, 5, 2, 5, 4, 5,\n",
       "       4, 5, 4, 4, 5, 4, 5, 4, 5, 4, 2, 4, 1, 4, 4, 4, 4, 5, 4, 4, 1, 4,\n",
       "       4, 4, 2, 5, 4, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 2, 4, 5,\n",
       "       4, 4], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "800e9e3b-1b0e-498a-830a-eb93799921fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([200, 6])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_onehot = tf.one_hot(predictions, depth=6, dtype=tf.int32)\n",
    "predictions_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24c75ccf-ed0c-41c1-8a3a-fa96dc2e5974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 6), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0]])>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_onehot[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "444e4d1b-3df2-4cfb-8b00-87726f2b0bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "to_categorical(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a658a5af-20c5-48bd-8051-b32e0c3928f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        79\n",
      "           1       0.62      0.48      0.54        27\n",
      "           2       0.69      0.55      0.61        33\n",
      "           3       0.00      0.00      0.00        15\n",
      "           4       0.05      0.50      0.08        10\n",
      "           5       0.77      0.75      0.76        36\n",
      "\n",
      "   micro avg       0.32      0.32      0.32       200\n",
      "   macro avg       0.35      0.38      0.33       200\n",
      "weighted avg       0.34      0.32      0.31       200\n",
      " samples avg       0.32      0.32      0.32       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_true, y_pred=predictions_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5f44296b-ddc7-4efc-b2ff-235b9fb442be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.load_weights('models/robosweights1k/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "baa5c863-9a11-4d6b-8c18-7d70da25782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.predict(test_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
